\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[galician]{babel}
\usepackage{csquotes}

\usepackage{array}
\usepackage{booktabs}            % \toprule \midrule \bottomrule
\usepackage{tabularx}   
\usepackage{verbatim}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{multicol}
\tcbuselibrary{breakable}
\usepackage[style=ieee,backend=bibtex]{biblatex}  % o style=apa, ieee, etc.
\addbibresource{referencias.bib}

\makeatletter
\let\@makecaption\relax
\makeatother

% Fonts: Palatino for body, Helvetica for headers
\usepackage{mathpazo} % Palatino
\usepackage{helvet}
\usepackage{microtype}

% Decorative initials
\input Eichenla.fd
\newcommand*\initfamily{\usefont{U}{Eichenla}{xl}{n}}
\usepackage{lettrine}

% Colors
\usepackage{xcolor}
\definecolor{techblue}{RGB}{0,51,102}
\definecolor{rosa}{RGB}{196,45,137}
\definecolor{lightgray}{RGB}{100,100,100}

% Headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textsf{\nouppercase{\leftmark}}}
\fancyhead[R]{\small\textcolor{lightgray}{\thepage}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}

% Chapter and section styling
\usepackage{titlesec}
\titleformat{\chapter}[hang]
  {\normalfont\LARGE\bfseries\sffamily\color{rosa}}
  {\thechapter.}{15pt}{}
\titlespacing*{\chapter}{0pt}{-20pt}{30pt}

\titleformat{\section}
  {\normalfont\Large\bfseries\sffamily\color{rosa}}
  {\thesection}{1em}{}

\titleformat{\subsection}
  {\normalfont\large\bfseries\sffamily}
  {\thesubsection}{1em}{}

% Better spacing
\usepackage{setspace}
\setstretch{1.1}
\setlength{\parskip}{0.4em}
\setlength{\parindent}{15pt}

% Table of contents styling
\usepackage{tocloft}
\renewcommand{\cfttoctitlefont}{\LARGE\bfseries\sffamily\color{rosa}}
\renewcommand{\cftchapfont}{\bfseries\sffamily}
\renewcommand{\cftsecfont}{\sffamily}

% Hyperlinks
\usepackage[colorlinks=true,linkcolor=rosa,citecolor=techblue,urlcolor=techblue]{hyperref}


\title{\scshape\Huge\color{rosa}Chatbot para a documentación e normativa da UDC}

\author{
Marcelo Ferreiro Sánchez \\ 
\textit{Portavoz do grupo} \\
\texttt{marcelo.fsanchez} -- \texttt{marcelo.fsanchez@udc.es}
\and
Marcos Grobas Martínez \\
\texttt{marcos.grobas} -- \texttt{marcos.grobas@udc.es}
\and
José Romero Conde \\
\texttt{j.rconde} -- \texttt{j.rconde@udc.es}
}

\date{}

\begin{document}

\pagenumbering{roman}
\maketitle

\begin{abstract}
\noindent 
O obxectivo do proxecto é desenvolver un chatbot capaz de solventar dúbidas
acerca do funcionamiento dos procesos burocráticos e de documentación da
Universidade da Coruña (UDC). Para conseguilo optamos por unha arquitectura
xenerativa aumentada por recuperación (RAG), técnica moi empleada para mellorar
o desempeño de chatbots baseados en LLM cando buscan información específica dun
dominio sobre o que o modelo de linguaxe orixinal non foi entrenado. 
\end{abstract}

\tableofcontents
\clearpage

\pagenumbering{arabic}
\chapter{Introdución}
\lettrine[lines=3,lraise=0.1,findent=2pt,nindent=0em]{\initfamily{A}}{}burocracia
de calquera campo pode chegar a ser moi complexa e pode chegar a consumir moito
tempo e recursos ás persoas que teñen que lidiar con ela. Os procesos
universitarios non son unha excepción e moitas das persoas involucradas neles
(sexan, alumnos, profesores ou persoal administrativo) os poden atopar
inabarcables ou imposibles de navegar sen axuda.

Neste contexto, apreciouse como podería ser de gran utilidade o desenvolvemento 
dun chatbot capaz de responder a preguntas relacionadas coa documentación e 
normativa da Universidade seguindo a gran tendencia da actualidade de empregar 
chatbots para diversas tarefas.

O obxectivo deste proxecto é desenvolver un chatbot que poida simplificar e 
explicar \textbf{referindo sempre ás fontes burocráticas oficiais} os procesos 
burocráticos e de documentación da Universidade da Coruña (UDC).

O sistema é resultado da unión de compoñentes e técnicas xa ben coñecidas, 
sempre tendo en mente o obxectivo a cumprir. Polo tanto, tratouse máis ben 
dunha tarefa de aplicación e adaptación de técnicas e ferramentas xa existentes 
nun caso particular, antes que de deseño ou resolución de novos problemas.

Se ben este traballo está circunscrito no contexto da asignatura de Técnicas 
Avanzadas de Procesamiento de Linguaxe Natural (TAPLN), debido á súa natureza 
e obxectivo, gran parte do tempo invertido no proxecto dedicouse á tarefa de 
recolección de información para o RAG.

Non existindo unha 'base de datos' oficial da UDC sobre a que un usuario poda 
descargar toda a documentación relativa ao centro, senón que atópase esta 
repartida nas páxinas web dos seus diferentes centros, foi necesario deseñar 
unha solución que nos permita extraela a partir dos seus portais oficiais.

Este tipo de tarefas non son novas no mundo da informática, de feito pertencen 
a unha área máis que consolidada chamada Recuperación de Información (IR) e 
tales métodos que buscan, extraen e organizan información disposta en webs html 
son coñecidos como \emph{crawlers}, parte esencial do traballo final pois para 
asegurar que o sistema sexa capaz de responder á maioría de dúbidas dos 
usuarios, é necesario ter un corpus de toda información mínimamente relevante 
acerca do funcionamento interno da universidade.




\chapter{Solución proposta}
\lettrine[lines=3,lraise=0.1,findent=2pt,nindent=0em]{\initfamily{M}}{}ostrarase
nesta sección a arquitectura xeral do sistema proposto e, a continuación,
describirase en detalle cada un dos seus compoñentes principais, sendo estes o
\emph{Crawler} e o \emph{RAG system}. O obxectivo desta arquitectura é combinar
un mecanismo de adquisición automática de información con un modelo de
linguaxe capaz de xerar respostas fundamentadas en contido externo e actualizado.

\section{A Arquitectura}
De maneira xeral, o sistema componse de dúas partes funcionalmente diferenciadas
pero fortemente interconectadas. Por unha banda, o sistema de \emph{crawling},
encargado de explorar e percorrer de forma automatizada os distintos portais e
páxinas web da Universidade da Coruña (UDC), co fin de descargar e
almacenar aqueles documentos que potencialmente conteñen información relevante.
Por outra banda, o \emph{RAG system}, concibido como unha pipeline de
procesamento da información que integra mecanismos de recuperación e xeración,
é o responsable de atender as consultas dos usuarios e producir respostas en
linguaxe natural apoiadas en contido externo recuperado. \\ \\ Nas siguientes subseccións explicaranse en detalle o funcionamento interno de
cada parte e cómo interactúan entre elas. Optouse
por unha orde de exposición que siga o fluxo de execución do sistema, comezando
polo \emph{Crawler} e continuando polo \emph{RAG system}. Aínda que se considerou
presentar o desenvolvemento seguindo a orde cronolóxica da implementación,
descartouse esta opción ao introducir unha carga expositiva innecesaria e
dificultar a comprensión global da arquitectura. De calquera
forma, explicarase a evolución das partes que máis cambios sufriron ao longo da
súa implementación.



\section{O \emph{Crawler}} Considerouse que a posesión dun bo volume de datos
sería crucial para resolver o problema a tratar, na maioría dos casos as dúbidas
burocráticas resólvense encontrando o documento adecuado, e de non telo, o
sistema veríase obrigado a comuinicarlle ao usuario de que non dispón da información solicitada, limitando
significativamente a súa capacidade para ofrecer respostas útiles. É por iso que a
Recuperación de Información xoga un papel crucial para o RAG. 

\subsection{Selección de páxinas relevantes}
Sendo así, o primeiro problema a abordar é o deseño dun criterio axeitado para
determinar que se considera un \textbf{documento relevante} no contexto da
aplicación. Trátase dunha cuestión que pode complicarse \textit{ad infinitum}
e que continúa a ser unha área de estudo activa na literatura recente
\cite{Pezzuti_2025}. Porén, no marco deste traballo optouse por un enfoque moito
máis sinxelo e pragmático.

Esta decisión baséase na premisa de que a tarefa non require o manexo de
conxuntos masivos de datos nin a exploración exhaustiva da web. Abonda con
navegar un número limitado de URLs, principalmente pertencentes aos
directorios raíz das distintas facultades e servizos da UDC, polo que resulta
aceptable descargar páxinas e documentos potencialmente pouco relevantes.
Ademais, o volume total de información a recoller presenta un límite de tamaño finito e razoable. Mesmo nun escenario extremo no que se descargasen todas as URLs asociadas á UDC, o espazo de almacenamento necesario estaría moi lonxe das ordes de magnitude propias dun \textit{crawler} de propósito xeral, onde si se require xestionar volumes de datos a escala de terabytes. Esta abordabilidade é posible grazas a que, unha vez descargadas, as páxinas e documentos son procesados de forma estruturada, permitindo unha xestión eficiente do contido.

Deste xeito, a solución proposta concíbese como un método sinxelo pero funcional,
baseado nunha asignación de relevancia binaria (\emph{relevante} ou
\emph{non relevante}) a partir de \textit{keywords}. A pesar da súa simplicidade,
este enfoque está estreitamente relacionado cos métodos pioneiros de
\textit{focused crawling} \cite{Chakrabarti1999}, que sentaron as bases para a
recuperación de información temática na web.

As palabras clave seleccionadas son fixas e específicas do ámbito académico e
do vocabulario burocrático da universidade, aparecendo en galego, castelán e
inglés. Para consultar a lista completa, véxase o Apéndice~\ref{app:keywords}.


\subsection{Procesado dos contidos de cada páxina}
Unha vez unha páxina se considera relevante, é necesario procesar o seu
contido para presentalo, sempre que sexa posible, en texto plano, evitando
decoradores e elementos innecesarios --como menús, cabeceiras, pés de páxina, publicidade ou elementos visuais que non aportan contido relevante--. Para iso, o \emph{crawler} encárgase de
\emph{parsear} a estrutura da páxina, extraendo o contido textual relevante
de HTML e outros elementos web. No caso de uso deste traballo, moita
información burocrática útil para o usuario final atópase en documentos PDF
ligados ao URL; por iso, o \emph{crawler} tamén se encarga de extraer, parsear e converter estes ficheiros a texto plano. Dispoñer do contido en texto plano permite posteriormente dividir os documentos en \emph{chunks}, xerar \emph{embeddings} e construír \emph{vector stores}, permitiendo así a recuperación e xeración eficiente de respostas no sistema RAG.


Non obstante, este enfoque presenta unha limitación importante: calquera
información que apareza exclusivamente en formato de imaxe (capturas de
pantalla, carteis dixitalizados ou documentos escaneados) permanece
invisible para o \emph{crawler}. Esta debilidade fíxose evidente nas primeiras
fases de probas do \emph{chatbot}, cando o sistema non foi capaz de responder
a unha pregunta básica para estudantes: ``Cantos libros pódense
emprestar?''. Tras investigar o caso e verificar que a
\href{https://www.udc.es/es/biblioteca/servizos/prestamo/}{páxina correspondente}
fóra efectivamente analizada polo \emph{crawler}, comprobouse que, aínda que a
información figura nunha táboa dentro da páxina, esta estaba en formato
imaxe (véxase \ref{figprestamos}). Este exemplo evidencia que é necesario
aproveitar mellor a información dispoñible en cada páxina para cumprir os
obxectivos establecidos.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{imaxes/prestamos.png}
    \caption{ Táboa de préstamos}
    \label{figprestamos}
\end{figure}


\subsubsection{Procesado de imaxes mediante OCR}
O recoñecemento óptico de caracteres (OCR, polas súas siglas en inglés) é unha
técnica que permite extraer texto a partir de imaxes dixitais. A súa aplicación
máis habitual é a dixitalización de documentos físicos escaneados, mais tamén
pode empregarse en calquera imaxe que conteña caracteres, como é o caso neste
traballo. Para este fin, implementouse unha interface en \textit{Python}
empregando a librería \textit{pytesseract}, que actúa como envoltorio do motor
OCR \texttt{Tesseract} \cite{tesseract_ocr_github}, inicialmente creado por HP e
actualmente mantido por Google.

Desde o punto de vista computacional, o procesado mediante OCR resulta asumible.
A maioría das imaxes presentes nos documentos son pequenas e irrelevantes, polo
que a súa análise non introduce unha latencia significativa no sistema. A modo
ilustrativo, o procesado OCR dunha imaxe tabular real (Figura~\ref{figprestamos})
presentou un tempo de execución de $0.6280$ segundos, o que confirma a
viabilidade práctica desta aproximación.

Durante o desenvolvemento do sistema observouse que unha parte relevante da
información aparece en forma de táboas embebidas como imaxes. Nunha primeira
aproximación, estas imaxes procesábanse mediante OCR xeral, obtendo un texto
plano sen estrutura explícita. Porén, este enfoque presentaba unha limitación
importante: ao fragmentar posteriormente o texto en \textit{chunks}, a
estrutura lóxica da táboa perdíase, facendo que os fragmentos resultantes non
contivesen contexto suficiente para unha interpretación correcta.

Para mitigar este problema, adoptouse un enfoque de OCR en dúas modalidades.
Por unha banda, incorpórase un OCR especializado na detección de estruturas
tabulares, capaz de identificar filas, columnas e relacións internas. Por outra
banda, mantense un OCR xeral para aquelas imaxes que non presentan unha
estrutura tabular clara, como infografías ou imaxes con texto libre. Este deseño
permite extraer información textual de forma robusta e adaptada á natureza de
cada imaxe, preservando a estrutura cando esta existe e contribuíndo a eficiencia
global do sistema.

\subsection{Funcionamento interno do \textit{Crawler}}

A implementación proposta usa da librería \textit{Python BeautifulSoup} para
navegar a rede e extraer información das páxinas. O proceso comeza cunha lista
de URLs semente que representa os portais principais da universidade
(concretamente a \textit{homepage} de cada facultade). Para cada URL, o sistema
descarga o contido HTML, extrae todos os enlaces e imaxes presentes na páxina, e
identifica documentos PDF que conteñan termos relacionados coa burocracia
universitaria segundo a lista de \textit{keywords} antes exposta. Os recursos
descargados almacénanse de forma organizada no sistema de ficheiros local,
mentres que un sistema de metadatos rexistra información sobre cada recurso para
optimizar futuras execucións, concretamente garda o \textit{etag}, a data de
última modificación de cada páxina, a data de descarga e o hash do contido.

En canto ás imaxes, aplícase OCR a todas as presentes en cada documento. O
proceso realízase en dúas fases consecutivas: primeiro procésase coa libraría
\texttt{img2table} (deseñada para extraer texto de táboas en imaxes) e, se esta
primeira fase non extrae un número mínimo de caracteres, entón aplícase un
segundo OCR utilizando \texttt{pytesseract} de xeito estándar. O texto extraído
por OCR intégrase directamente no contido textual da páxina á que pertence a
imaxe (sempre e cando alcance un número mínimo de caracteres definido): engádese
ao ficheiro de texto correspondente cun \textit{header} que indica a súa orixe
(\texttt{[TÁBOA OCR]} ou \texttt{[IMAXE OCR]}), permitindo así conservar tanto
o texto HTML como o contido visual nun único documento estruturado.

Todo este procesamento realízase ao final de \textit{crawlear} cada páxina:
primeiro extráense as hiperreferencias a outras páxinas e documentos PDF, e
despois procesanse as imaxes encoladas para OCR. Esta estratexia minimiza o
cuello de botella que se produciría se o OCR se realizase de forma síncrona
mentres se descargan outros recursos.

Para acelerar o proceso de \textit{crawling} aplicáronse técnicas de programación
concurrente. É posible establecer un número de traballadores (\textit{workers})
aos cales se lles asigna unha URL como punto de partida, permitindo múltiples
crawlers simultáneamente navegando a rede. Para evitar problemas de escritura
nos ficheiros de metadatos ou rexistro de páxinas visitadas, implementáronse
\textit{locks}, garantindo que dous crawlers non poidan escribir á vez nun mesmo
recurso.

Antes de procesar cada documento, comprobanse os metadatos para determinar se o
contido cambiou desde o último crawleo. Primeiro verifica os campos
\texttt{last\_modified} e \texttt{etag}; se se detecta unha modificación, extráese
o texto completo e calcúlase un hash do contido. Se este hash é diferente do
hash previo, a variable booleana \texttt{needs\_embeddings} actívase, indicando
que o documento debe ser vectorizado no seguinte paso. Deste xeito, evítase xerar
embeddings para documentos que non cambiaron, optimizando tanto o tempo de
procesamento como o uso de recursos.

En conxunto, esta combinación de crawling distribuído, OCR en dúas fases, xestión
eficiente de imaxes e metadatos, e comprobación de cambios mediante hash garante
que o crawler funcione de forma rápida, escalable e eficiente, mantendo a
información relevante organizada e lista para o procesamento posterior, como se
mostra nas gráficas do apéndice~\ref{fig:crawlerRun}.


\section{Análise dos datos obtidos mediante o \textit{Crawler}} 

Tras facer un \textit{crawl} sobre as \textit{homepages} das facultades e escolas da UDC, obtiveronse un total de 4.966 documentos, dos cales 2.668 son páxinas \textit{HTML} e 2.298 documentos \textit{PDF} (véxase~\ref{fig:file_types_distribution}).  

Debido á relación coas técnicas clásicas de Recuperación da Información, realizouse unha análise exploratoria do vocabulario e da lonxitude dos documentos: 

\begin{table}[h]
\centering
\caption{Estadísticas del corpus}
\label{tab:corpus_stats}
\begin{tabular}{lr}
\hline
\textbf{Métrica} & \textbf{Valor} \\
\hline
Arquivos procesados & 4.966 \\
Total de caracteres & 75.088.461 \\
Total de palabras & 10.164.265 \\
Tamaño del vocabulario & 73.640 \\
Palabras únicas (hapax legomena) & 15.889 \\
\hline
\end{tabular}
\end{table}

Atopáronse 10.164.265 palabras cun vocabulario de 73.640 termos, das cales 15.889 aparecen só unha vez (\textit{hapax legomena}), un 20\% do vocabulario. A maioría non son erros: só 230 son faltas ortográficas e 30 xeradas polo OCR. Este fenómeno de vocabulario de uso desigual segue a lei de Zipf, onde poucas palabras son moi frecuentes e moitas son raras, como se mostra nas Figuras~\ref{figZipf} e~\ref{figEDA}.  


Tamén se observa o principio de Pareto: aproximadamente o 20\% das palabras máis frecuentes representan arredor do 80\% das ocorrencias, aínda que neste corpus a relación é máis extrema: o 2,1\% do vocabulario alcanza o 80\% das ocorrencias e o 20\% cobre o 97,5\%.  

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imaxes/zipf.png}
    \caption{Frecuencia de termos en diversas linguas \cite{wikipedia_zipf}}
    \label{figZipf}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imaxes/analisis_corpus.png}
    \caption{Gráficos de distribución de lonxitude de documentos e frecuencia de termos}
    \label{figEDA}
\end{figure}

A distribución das lonxitudes dos documentos (Figura~\ref{figEDA}, esquerda) mostra moitos documentos curtos: mediana de 4.200 caracteres e media de 15.121, indicando asimetría debido a documentos longos que elevan a media. Moitas páxinas web teñen contido limitado ou só mostran ligazóns ou imaxes. No \textit{crawleo} detectáronse 115 páxinas baleiras (0 caracteres), ben por conter só imaxes sen texto extraíble por OCR, ben por ser redireccións ou navegación pura.









\section{O \textit{RAGsystem}}

O sistema RAG (Retrieval-Augmented Generation) implementado constitúe o núcleo
do chatbot, combinando técnicas de recuperación de información con modelos de
linguaxe xerativos. O proceso de resposta a unha consulta desenvólvese nun fluxo
secuencial e optimizado:

Inicialmente, a consulta do usuario pasa por un módulo de \textit{clasificación}
realizado por un modelo de linguaxe lixeiro (Claude Haiku) para determinar se
require recuperación de documentación. Cando é necesaria, a mesma consulta é
\textit{reescrita} (\textit{query rewriting}) para mellorar a súa formulación
tanto para a busca semántica como léxica.

Para a recuperación, empregase un sistema \textit{híbrido} que combina busca
densa (mediante embeddings semánticos e similitude coseno en FAISS) e busca
léxica (usando BM25, unha evolución do TF-IDF). As puntuacións de ambos métodos
fusiónanse con pesos determinados empiricamente,
seleccionando os $k$ fragmentos de texto máis relevantes da base de documentos
da universidade.

Os documentos recuperados, xunto coa consulta orixinal e o historial de
conversación (de lonxitude configurable), intégranse nun prompt estruturado.
Este pasa a un modelo de linguaxe grande (Claude Sonnet) escollido pola súa
capacidade de razoamento e, especialmente, pola súa efectividade na comunicación
entre axentes e o seguimento de instrucións complexas
\cite{claude-agents-2024}. O modelo xera así unha resposta precisa
fundamentada na documentación oficial.

O sistema enriquece cada documento recuperado con metadatos que inclúen as súas
procedencias (hipervínculos), datas (de actualización da páxina), puntuacións de
relevancia tanto da busca densa como da léxica, garantindo total trazabilidade e
permitindo a verificación das fontes utilizadas en cada resposta.

\subsection{Construción da base de vectores}
\cite{lewis2020retrieval} A base de vectores (\textit{vectorstore}) constrúese
a partir dos documentos recollidos e previamente convertidos a texto plano
(.txt) polo \textit{crawler}. Cada documento é fragmentado en varios
\textit{chunks} de tamaño configurable, establecéndose tamén un \textit{overlap}
que especifica cantos \textit{tokens} comparten os fragmentos consecutivos. A
utilidade deste solapamento é impedir que referencias cruzadas ou ideas contidas
nun texto queden cortadas arbitrariamente ao final dun \textit{chunk}, xa que
nese caso o LLM non podería comprender o contexto completo.

Cada fragmento de texto (\textit{chunk}) xera, ademais do seu contido, un
conxunto de metadatos que inclúen: o hipervínculo de orixe da páxina, a data da
última actualización do documento orixinal (obtida durante o \textit{crawling})
e outros campos auxiliares para a trazabilidade. Estes metadatos almacénanse
xunto co vector resultante e son recuperados co texto durante a busca, o que
permite ao sistema referenciar a fonte exacta e avaliar a vixencia da
información.

Para o almacenamento e indexación eficiente dos \textit{embeddings}, emprégase
FAISS (\textit{Facebook AI Similarity Search}) \cite{johnson2019billion}, unha
biblioteca optimizada para a busca de similaridade en espazos vectoriais de alta
dimensionalidade. FAISS implementa algoritmos de busca aproximada de veciños
máis próximos (ANN, \textit{Approximate Nearest Neighbors}), que permiten
realizar consultas en millóns de vectores de forma eficiente. O proceso funciona
do seguinte xeito: cada \textit{chunk} de texto convértese nun vector denso
(\textit{embedding}) mediante un modelo de linguaxe especializado; estes
vectores almacénanse, xunto cos seus metadatos, nunha estrutura de índice
optimizada; e cando se realiza unha consulta, FAISS calcula rapidamente os
vectores máis similares utilizando medidas de distancia como a similaridade
coseno ou a distancia euclidiana~\cite{douze2024faiss}.






\subsection{O sistema de recuperación}

O sistema polo que o RAG recibe os documentos relevantes para cada consulta
recibe múltiples iteracións e cambios tras varias fases de testeo e
replanteamento teórico. Trátase dun compoñente clave do sistema e non ten unha
solución trivial, pois é un área de investigación moi activa
\cite{systematic_rag_2025}. Poderíase dicir que, aínda que a área de RI
(Recuperación de Información) parecía estar algo estancada nos últimos anos
debido aos bos resultados que alcanzaron os buscadores web, agora volve a verse
moi activa grazas ao xurdimento dos sistemas RAG.

Inicialmente, o sistema empregou un método de recuperación baseado
exclusivamente na similitude coseno sobre vectores FAISS. Porén, durante as
primeiras probas comprobouse que esta aproximación podía non ofrecer os
resultados esperados, xa que daba a mesma importancia a todas as palabras da
consulta, sen ter en conta cal podería ser a palabra clave máis relevante. A
maioría das consultas, especialmente aquelas de carácter burocrático, conteñen
moitas palabras comúns e repetidas ao longo da colección de documentos (como
\textit{proceso}, \textit{documento} ou \textit{normativa}), o que podía facer
que o sistema recuperase documentos pouco relevantes.

Como primeira mellora, implementouse MMR (Maximal Marginal Relevance) en lugar
da similitude coseno. A idea subxacente era que, deste xeito, non se
recuperarían documentos redundantes entre si, xa que se existisen varios
documentos moi similares, a súa información adicional para a consulta sería
limitada. Non obstante, durante a experimentación observouse que esta suposición
non se cumpría no noso dominio. É moi común atopar diferentes documentos
oficiais que repiten a mesma información, ou varias versións e revisións dun
proceso en distintos documentos. Neses casos, o sistema tendía a devolver só un
deles, perdéndose información relevante ou, incluso, indicando un proceso xa
obsoleto, ao priorizar a diversidade fronte á exhaustividade
\cite{carbonell1998mmr}. Esta problemática, relacionada coa vixencia temporal
dos documentos, constitúe un dos maiores retos deste proxecto (e de calquera
sistema RAG que manipule información cambiante ao longo do tempo
\cite{grofsky2025freshness}) e será tratada con maior detalle máis adiante, xa
que require modificacións en diversas partes do sistema.

Unha vez comprobado que o MMR non era axeitado para o noso dominio, retomouse o
enfoque orixinal, pero buscando unha maneira de discriminar mellor entre os
termos relevantes e irrelevantes da consulta. Para iso, decidiuse explorar
métodos clásicos de recuperación, como o TF-IDF (Term Frequency – Inverse
Document Frequency) \cite{sparck_jones1972}. A primeira aplicación consistiu
en usalo para reordenar (\textit{reranking}) os documentos xa recuperados por
FAISS. Con todo, axiña se observou que esta estratexia non ofrecía resultados
óptimos, xa que simplemente cambiaba a orde dos documentos recuperados, sen
engadir novos documentos relevantes que puidesen ser ignorados pola similitude
coseno.

A solución final adoptada foi un sistema híbrido que combina as puntuacións da
similitude coseno (recuperación densa) e do BM25 (unha mellora sobre o TF-IDF
para recuperación léxica ou dispersa), mediante pesos configurables. Despois
dunha avaliación da literatura, no inicio, estableceuse un balance de 50/50 entre
ambos métodos, un enfoque común en sistemas RAG actuais
\cite{regulatory_hybrid_2025}, \cite{adobe_hybrid_2024}, máis unha vez
deseñado un \textit{pipeline} para realizar validación, optouse por usar tal
experimento para determinar empíricamente o mellor valor de peso, ademáis do
mellor \textit{chunk size}.

Ademais, o fluxo de recuperación intégrase dentro dunha canle máis ampla de
procesamento da consulta. Antes de realizar a búsqueda, lévasse a cabo unha
\textit{clasificación da consulta} (mediante un LLM lixeiro, como Claude Haiku)
para determinar se é necesario recuperar documentos. En caso afirmativo,
aplícase un \textit{reescrito da consulta} (\textit{query rewriting}), co
obxectivo de optimizar a súa formulación para a busca tanto no índice denso
(FAISS) como no léxico (BM25). Os documentos recuperados por este sistema
híbrido, xunto coa consulta orixinal, pásanlle finalmente a un modelo de
linguaxe máis grande (como Claude Sonnet) para xerar a resposta final.





\section{Os modelos}

A elección dos modelos de linguaxe no sistema RAG realizouse atendendo a
criterios de eficiencia, latencia e rendemento específico para cada tarefa do
fluxo. O sistema emprega de xeito estratéxico modelos de diferentes capacidades
segundo a súa función dentro da arquitectura, optimizando así o balance entre
precisión, tempo de resposta e custo.

Para as tarefas iniciais de procesamento da consulta (clasificación para
determinar a necesidade de recuperación de documentos e reescritura
(\textit{query rewriting}) para mellorar a formulación da busca) utilízase
\textbf{Claude Haiku}. Este modelo, sendo o máis rápido e económico da familia
Claude de Anthropic, resulta adecuado para tarefas lixeiras de comprensión e
transformación textual que non requiren razoamento profundo pero si baixa
latencia.

Para a fase central de xeración da resposta empregase \textbf{Claude Sonnet}.
Este modelo ofrece un equilibrio óptimo entre capacidades de razoamento,
seguimento de instrucións complexas e eficiencia, sendo especialmente adecuado
para sintetizar información de múltiples documentos e producir respostas
coherentes, precisas e ben fundamentadas. A súa capacidade para traballar con
\textit{prompts} estruturados que inclúen contexto recuperado, historial de
conversación e instrucións específicas de formato resulta clave para a calidade
final do sistema.

A separación de responsabilidades entre modelos lixeiros para tarefas previas e
modelos máis capaces para a xeración final segue unha práctica común en
arquitecturas de sistemas RAG, destinada a minimizar a latencia e o custo
computacional sen comprometer a calidade das respostas
\cite{lakshmanan2024rag}, \cite{gao2023retrieval}. Neste proxecto, a elección
de Claude Haiku e Sonnet implementa esta estratexia adaptándoa aos recursos e
limitacións do noso entorno.


\section{Ferramentas usadas}

\chapter{Instalación e uso}





\chapter{Validación e resultados}
% \lettrine[lines=3,lraise=0.1,findent=2pt,nindent=0em]{\initfamily{R}}{}esults presentation here.
\section{Validación do sistema RAG}

\subsection{Complexidade da validación}
A validación dun sistema Retrieval-Augmented Generation (RAG) é intrinsecamente
complexa debido á súa natureza híbrida, xa que combina dous subsistemas
conceptualmente distintos pero fortemente acoplados: o módulo de
\textit{retrieval}, encargado de seleccionar a información relevante, e o módulo
de \textit{generation}, responsable de producir a resposta final condicionada
polo contexto recuperado. Avaliar correctamente un sistema RAG implica analizar
de forma separada e conxunta ambos compoñentes, así como a súa interacción,
incrementando notablemente a dificultade do proceso de validación
\cite{rag_eval_survey_2024}.

Ademais, unha avaliación rigorosa require a disposición de conxuntos de datos
anotados e métricas específicas para cada etapa do pipeline. A literatura sinala
que a definición de \textit{ground truth} fiable —cun conxunto completo de
documentos relevantes e respostas correctas— é custosa en tempo e recursos, e
que as métricas existentes poden verse afectadas pola variabilidade na anotación
humana \cite{rag_eval_survey_2024,llm_judge_eval_2024}.

\subsection{Conxunto de validación}
Para realizar unha avaliación práctica e reproducible dentro das limitacións do
proxecto, construíuse un conxunto de validación reducido composto por \textbf{25
preguntas}. Cada pregunta está asociada a unha resposta esperada e ao documento
concreto do corpus onde se atopa a información necesaria para resolvela. Na
maioría dos casos, a información relevante localízase de forma explícita nun
único documento.

É importante destacar que esta avaliación realízase sobre un \textbf{corpus
reducido e controlado de documentos}. Este corpus representa só unha fracción do
conxunto total de documentos que o sistema RAG manexará nun contexto real, onde
se espera que haxa moitos máis documentos, algúns dos cales poderían recuperarse
sen ser necesarios para responder a consulta concreta. Traballar cun corpus
acotado permite, con todo, illar variables e analizar de forma precisa o
comportamento do sistema. En particular, pódese estudar o impacto de parámetros
como estratexias de recuperación, métricas de similitude, tamaño de chunk ou
número de documentos recuperados, aspectos relevantes no deseño práctico de
sistemas RAG \cite{rag_required_abilities_2023}.


Esta aproximación debe considerarse \textbf{subestimada e conservadora}, xa que
en escenarios reais poderían existir múltiples documentos relevantes por
consulta non cubertos neste estudo. As métricas obtidas non buscan medir o
rendemento absoluto do sistema en produción, senón servir como base experimental
para estudar o impacto relativo das distintas decisións de deseño do RAG.

\subsection{Evaluación do módulo de \textit{retrieval}} O módulo de recuperación
avalíase de forma independente mediante métricas clásicas de \textit{Information
Retrieval}, adaptadas ao contexto dun sistema RAG:

\begin{itemize}
    \item \textbf{Recall@10}: proporción de documentos relevantes que aparecen
    entre los 10 primeros resultados recuperados. Calculase como a fracción de
    documentos relevantes presentes no top-10 do ranking. Aínda que presentamos
    principalmente o \textbf{recall fraccional}, convén aclarar que neste
    conxunto de validación cada pregunta adoita ter un único documento
    relevante. Polo tanto, nesta situación o recall fraccional é practicamente
    equivalente ao \textbf{recall binario}, que indica se polo menos un
    documento relevante aparece entre os top-10. Esta métrica reflicte a
    capacidade do sistema de garantir que a información esencial chega ao modelo
    xerativo.
    \item \textbf{MRR (Mean Reciprocal Rank)}: penaliza a aparición tardía do
    primeiro documento relevante no ranking. Calculase como o recíproco da
    posición do primeiro documento relevante atopado (1 se é o primeiro, 0.5 se
    é o segundo, etc.). Esta métrica reflicte a rapidez coa que o sistema
    proporciona información útil ao LLM.
    \item \textbf{Precision@10}: proporción de \textit{chunks} recuperados que
    pertencen a documentos relevantes dentro dos 10 primeiros resultados. Avalía
    a capacidade do sistema para minimizar a recuperación de información
    irrelevante, especialmente crítico en corpus grandes ou ruidosos. Esta
    métrica é a nivel de chunk, reflectindo o nivel de “ruído” que chega ao LLM.
\end{itemize}

No noso sistema, o \textbf{recall e o MRR} calcúlase a nivel de documento,
considerando cada documento como unha unidade discreta de relevancia. Isto
simplifica a avaliación e garante que se mida a efectividade do sistema en levar
información esencial ao modelo xerativo, independentemente de como os documentos
estean fragmentados en chunks para a súa utilización polo LLM. 

Pola súa banda, a \textbf{precision} calcúlase a nivel de chunk, como proporción
de fragments pertencentes a documentos relevantes entre os recuperados. Este
enfoque mixto (recall a nivel documento, precision a nivel chunk) proporciona
unha visión útil para o desempeño do sistema RAG: asegura que a información
clave está dispoñible para o LLM, ao mesmo tempo que avalía a calidade do
material recuperado.

\subsection{Evaluación da xeración}
A avaliación do módulo de xeración céntrase en dúas dimensións complementarias:

\begin{itemize}
    \item \textbf{Answer Relevance}: mide se a resposta aborda correctamente a
    pregunta e evita información irrelevante. Esta métrica non xulga a
    veracidade, só a pertinencia.
    \item \textbf{Answer Faithfulness}: avalía se as afirmacións contidas na
    resposta están efectivamente respaldadas polo contexto recuperado polo
    módulo de \textit{retrieval}, detectando posibles alucinacións ou
    información non soportada.
\end{itemize}

\subsubsection{Mecanismo de avaliación: \textit{LLM-as-judge}} Para automatizar
a avaliación empregouse un modelo de linguaxe avanzado (\textbf{GPT-5.2}) como
avaliador (\textit{LLM-as-judge}), garantindo capacidade suficiente para xulgar
de forma coherente e consistente as respostas do xerador
\cite{llm_judge_eval_2024}. Este enfoque permite unha avaliación máis detallada
que as métricas clásicas, incorporando aspectos de completitude, coherencia e
consistencia co contexto.

\paragraph{Faithfulness}
O LLM analiza cada resposta dividíndoa en afirmacións atómicas e comproba se
cada unha está efectivamente respaldada polo contexto recuperado. O score final,
entre 0 e 1, representa a proporción de afirmacións verificadas. Ademais, xérase
unha explicación detallada que identifica que afirmacións fallaron e por que,
permitindo unha maior transparencia na avaliación e a detección de posibles
alucinacións ou inclusión de información externa ao contexto.

\paragraph{Relevance}
A relevancia mide se a resposta aborda todos os puntos da pregunta de forma
concisa e útil, evitando información irrelevante ou redundante. Tamén retorna un
score entre 0 e 1 e unha explicación que describe posibles omisións ou exceso de
información.

\paragraph{Beneficios do enfoque}

O uso de \textit{LLM-as-judge} permite:
\begin{itemize}
    \item Avaliar fidelidade e pertinencia de respostas en linguaxe natural de forma consistente e reproducible.
    \item Detectar matices de completitude, coherencia e relevancia que métricas de IR tradicionais non capturan.
    \item Reducir a dependencia do etiquetado humano para a avaliación detallada de respostas.
\end{itemize}


\subsection{Uso do mecanismo de avaliación como métrica}
Anteriormente fíxose referencia a como empregariamos o \textit{pipeline} de
avaliación para atopar un máximo no espacio de hiperparámetros do modelo, sendo
estes o \textit{chunk size} e o peso entre a busca densa e a dispersa.

Tal experimentación requiriu dunha \textit{grid search} que mediu cinco métricas
para cada combinación de valores: tres propias de Recuperación de Información
(\textit{Recall}, \textit{Precision} e \textit{MRR}) e dúas propias da
avaliación con \textit{LLM-as-judge} (\textit{Faithfulness} e
\textit{Relevance}). Tras promediar os valores para cada pregunta, puidéronse
construir \textit{heatmaps} para cada unha das medicións (ver
Figura~\ref{fig:heatmap_metrics}).

métricas para valores máis altos do peso asignado a BM25 (busca léxica) fronte á
busca densa (FAISS). A puntuación final de cada documento no sistema híbrido 
calcúlase mediante a combinación lineal:

\begin{equation}
Score_{final} = Score_{denso} + \alpha \cdot Score_{disperso}
\label{eq:score_hibrido}
\end{equation}



onde $\alpha$ representa o peso relativo de BM25. Cando $\alpha > 1$, como no 
valor óptimo de 1.6 atopado, a busca léxica ten maior influencia na selección 
final. Isto indica que, para o dominio específico da documentación
burocrática universitaria, a recuperación baseada en coincidencia de termos
clave é especialmente efectiva. As consultas neste contexto adoitan incluír
nomes propios de formularios, códigos de procedementos ou termos técnicos moi
específicos, onde a recuperación léxica supera á semántica pura. Este fenómeno é
consistente coa literatura que destaca a superioridade de BM25 en dominios con
vocabulario especializado e baixa variación lexical \cite{lin2022lexical}.
Ademais, a natureza formal e estándar da linguaxe administrativa fai que os
documentos relevantes compartan termos exactos coas consultas, situación onde os
métodos de recuperación tradicional baseados en termos aínda ofrecen vantaxes
sobre os enfoques puramente semánticos.

Debido a que é necesario elixir un par de valores para os hiperparámetros,
optouse por aqueles que maximizan a métrica de \textit{Relevance} obtida
mediante \textit{LLM-as-judge}. Esta decisión baséase na premisa de que a
relevancia percibida da resposta final é o criterio de calidade máis alineado
coa experiencia do usuario final nun sistema de preguntas e respostas. Mentres
que métricas tradicionais de RI como \textit{Precision} e \textit{Recall} miden
a eficacia da recuperación de documentos, a \textit{Relevance} evalúa
directamente se o contido xerado responde de maneira útil e adecuada á intención
da consulta orixinal. Maximizar esta métrica prioriza que o sistema produza
respostas substanciais e directamente aplicables, por riba doutras
consideracións como a exhaustividade bruta (\textit{Recall}) ou a minimización
de información irrelevante (\textit{Precision}) no contexto recuperado. Esta
aproximación está avalada por investigacións recentes que suxiren que, en
sistemas RAG interactivos, a relevancia da resposta xerada é o predictor máis
forte da satisfacción do usuario \cite{saad2024evaluating}.

Polo tanto, tras facer un \textit{surface plot}
(Figura~\ref{fig:surface_plots_contour}) da \textit{Relevance} (e tamén da
\textit{Faithfulness}), optouse polo par de valores no seu cumio (que o
maximizan), sendo este un peso de 1.6 para BM25 e un \textit{chunk size} de 2048
\textit{tokens}. Tamén se realizou un \textit{heatmap} de todas estas métricas
promediadas (Figura~\ref{fig:heatmap_combined}), onde o par de valores con
mellor puntuación segue sendo o mesmo.

Non é de estrañar que o \textit{chunk size} óptimo para este dominio sexa
relativamente grande. A documentación burocrática e administrativa universitaria
adoita estar estruturada en seccións longas e autocontidas (como procedementos
completos, regulamentos ou guías), onde o contexto local é crucial para
comprender requisitos, excepcións ou pasos interrelacionados. Fragmentos
demasiado pequenos poden separar información conceptualmente unida, dificultando
que o modelo de linguaxe xere respostas coherentes e exhaustivas. Este resultado
é consistente coa literatura sobre recuperación en dominios técnicos e legais,
onde se recomiendan tamaños de fragmento maiores para preservar a integridade
semántica dos documentos \cite{wang2023chunking}.



\subsection{Consideracións sobre o corpus reducido}
O estudo realizouse sobre un \textbf{corpus reducido e controlado de
documentos}. Isto permite illar variables e analizar o impacto de parámetros
como: as estratexias de recuperación (TF-IDF, híbridas, reranking) ou o tamaño
de chunk e superposición

En escenarios de produción, con moitos máis documentos dispoñibles, é altamente
probable que o sistema recupere unha maior cantidade de documentos irrelevantes,
o que afectará especialmente á \textbf{precision@10}. O estudo realizado neste
corpus reducido proporciona unha visión inicial sobre a capacidade do RAG para
filtrar información relevante en presenza de ruído documental, e serve como base
experimental para estudar o impacto relativo das decisións de deseño.

\subsection{Limitacións}
O conxunto reducido introduce certas limitacións que afectan a toda a validación:
\begin{itemize}
    \item Non reflicte a heteroxeneidade nin a escala real do corpus.
    \item Subrepresenta casos que requiren integración multi-documento.
    \item Posibles omisións no etiquetado humano.
    \item As métricas non son directamente extrapolables a produción.
\end{itemize}

A avaliación proposta proporciona unha visión relativa do impacto das decisións
de deseño do RAG, e non debe interpretarse como unha medida directa do seu
rendemento absoluto en escenarios de produción \cite{rag_eval_survey_2024}.


\chapter{Conclusions e traballos futuros}

\section{Extensións arquitectónicas e metodolóxicas}

Ademais da ampliación das dimensións de avaliación, existen varias liñas de
mellora de arquitectura que poderían reforzar o rendemento e a robustez do
sistema RAG analizado neste traballo. En particular, a literatura recente
sinala que decisións relativas á segmentación do texto, ao reranking dos
documentos recuperados e á selección dos conxuntos de avaliación teñen un
impacto significativo na calidade final das respostas xeradas.

Unha primeira extensión relevante sería a adopción dunha estratexia de
\textit{small-to-big chunking}. Este enfoque consiste en empregar fragmentos
de tamaño reducido durante a fase de recuperación, co obxectivo de maximizar
a precisión e a discriminación semántica do retriever, e posteriormente
expandir eses fragmentos a unidades de contexto máis amplas na fase de
xeración. Deste modo, o modelo xerador dispón dun contexto máis rico e
cohesionado, preservando ao mesmo tempo a relevancia local identificada na
recuperación inicial. Estudos recentes sinalan que este tipo de estratexias
axudan a mitigar a perda de contexto e melloran a coherencia factual das
respostas, especialmente en tarefas de preguntas complexas \cite{wang2023chunking,wang-etal-2024-searching}.

Outra liña de traballo futuro consiste na incorporación dunha fase explícita
de \textit{reranking} baseada en modelos neuronais especializados, como
TILDEv2. Este tipo de modelos combina sinais léxicos e semánticos para refinar
a orde dos documentos recuperados, permitindo unha selección máis precisa do
contexto máis informativo antes da xeración. A integración dun reranker deste
tipo resulta especialmente relevante en escenarios con vocabulario técnico
ou especializado, nos que a recuperación puramente semántica pode resultar
insuficiente \cite{lin2022lexical}. Ademais, análises empíricas recentes mostran que a inclusión dunha etapa de
reranking pode producir melloras consistentes en métricas de recuperación e
na calidade percibida das respostas finais, mesmo cando se empregan
retrievers competitivos de base \cite{wang-etal-2024-searching}. A avaliación comparativa entre unha pipeline
sen reranking e outra que incorpore TILDEv2 permitiría cuantificar o impacto
desta capa adicional sobre diferentes métricas.

Finalmente, no ámbito da avaliación, unha extensión natural deste traballo
sería o uso de datasets estandarizados de \textit{benchmarking}, en
particular aqueles orientados a tarefas de \textit{fact checking} e
\textit{reasoning}. Este tipo de conxuntos de datos permiten avaliar non só a
corrección factual das respostas, senón tamén a capacidade do sistema para
razoar sobre evidencias múltiples e detectar inconsistencias ou información
incorrecta nos documentos recuperados. A súa incorporación facilitaría a
comparación directa con outros sistemas RAG descritos na literatura e
permitiría situar os resultados obtidos nun contexto máis amplo e
representativo do estado da arte, seguindo recomendacións metodolóxicas
recentes para a avaliación sistemática de arquitecturas RAG
\cite{rag_eval_survey_2024,wang-etal-2024-searching}.

En conxunto, estas liñas de traballo futuro apuntan cara a unha evolución do
sistema avaliado tanto a nivel arquitectónico como metodolóxico, reforzando a
súa robustez, capacidade de xeneralización e adecuación a escenarios reais de
uso. A súa exploración constitúe unha continuidade natural deste estudo e
abre a porta a avaliacións máis completas e comparables cos sistemas RAG
actuais descritos na literatura científica
\cite{rag_required_abilities_2023,lakshmanan2024rag}.

\section{Extensións da validación}
A avaliación presentada neste traballo céntrase en métricas clásicas de
recuperación e en criterios básicos de calidade da xeración, o cal resulta
adecuado para un estudo controlado cun conxunto de validación construído
manualmente. Esta elección responde a unha decisión metodolóxica orientada a
garantir reproducibilidade, trazabilidade e consistencia na avaliación, máis que
a unha limitación conceptual do enfoque.

A literatura recente sinala que o comportamento dun sistema RAG en escenarios
reais depende tamén dun conxunto de capacidades máis avanzadas (\textit{required
abilities}), cuxa avaliación resulta máis complexa e require condicións
experimentais específicas, como datasets deseñados ad hoc e un maior volume de
anotación humana \cite{rag_eval_survey_2024}.

Como liña de traballo futuro, resulta recomendable ampliar a validación cara á
avaliación destas capacidades, entre as que se inclúen:
\begin{itemize}
    \item \textbf{Noise robustness}: capacidade de xestionar documentos ruidosos semanticamente relacionados coa pregunta pero sen información útil para a resposta.
    \item \textbf{Negative rejection}: habilidade do sistema para recoñecer contextos insuficientes e evitar a xeración de respostas especulativas.
    \item \textbf{Information integration}: capacidade para combinar información procedente de múltiples documentos relevantes en preguntas complexas.
    \item \textbf{Counterfactual robustness}: aptitude para detectar e ignorar información incorrecta ou contraditoria presente nos documentos recuperados.
\end{itemize}

A incorporación destas dimensións permitiría unha avaliación máis completa do
sistema RAG, pero implicaría a ampliación do conxunto de validación actual ou a
creación de novos datasets específicos, así como un maior investimento en
anotación humana e deseño experimental. Estas extensións constitúen, por tanto,
unha continuidade natural deste traballo, orientada a aproximar a avaliación ás
condicións reais de uso e a analizar o comportamento do sistema en escenarios
máis complexos e variados \cite{rag_required_abilities_2023}.




\printbibliography
\appendix

\chapter{Máis información acerca do \textit{crawler}}

\section{\textit{Keywords} utilizadas polo crawler}
\label{app:keywords}

A continuación móstrase o conxunto completo de palabras clave utilizadas
polo \textit{crawler} para determinar a relevancia das páxinas:

\begin{multicols}{3}
\small
\begin{itemize}
    \item regulation
    \item reglamento
    \item normativa
    \item procedure
    \item procedimiento
    \item proceso
    \item form
    \item formulario
    \item solicitud
    \item guideline
    \item guia
    \item manual
    \item policy
    \item politica
    \item norma
    \item enrollment
    \item matricula
    \item inscripcion
    \item administrative
    \item administrativo
    \item academic
    \item academico
    \item calendar
    \item calendario
    \item syllabus
    \item programa
    \item requirements
    \item requisitos
    \item regulamento
    \item regulación
    \item procedemento
    \item solicitude
    \item guía
    \item docente
    \item asignatura
    \item política
    \item matrícula
    \item inscrición
    \item académico
    \item convocatoria
    \item prazo
    \item prazos
    \item documentación
    \item tramite
    \item trámite
    \item ordenanza
    \item resolución
    \item circular
    \item instrucións
    \item instrucciones
    \item bases
    \item anexo
    \item catalog
    \item catalogo
    \item catálogo
    \item library
    \item biblioteca
    \item collection
    \item coleccion
    \item colección
    \item acquisition
    \item adquisicion
    \item adquisición
    \item loan
    \item prestamo
    \item préstamo
    \item reserve
    \item reserva
    \item interlibrary
    \item interbibliotecario
    \item reference
    \item referencia
    \item circulation
    \item circulacion
    \item circulación
    \item periodical
    \item periodico
    \item periódico
    \item journal
    \item revista
    \item archive
    \item archivo
    \item arquivos
    \item repository
    \item repositorio
    \item classification
    \item clasificacion
    \item clasificación
    \item indexing
    \item indexacion
    \item indexación
    \item cataloging
    \item catalogacion
    \item catalogación
    \item dewey
    \item isbn
    \item issn
    \item bibliographic
    \item bibliografico
    \item bibliográfico
    \item holdings
    \item fondos
    \item serials
    \item publicacions seriadas
    \item special collections
    \item coleccions especiais
    \item reading room
    \item sala de lectura
    \item stacks
    \item depósito
    \item microfilm
    \item microficha
    \item digital library
    \item biblioteca dixital
    \item opac
    \item marc
    \item application
    \item deadline
    \item plazo
    \item documentation
    \item documentacion
    \item certification
    \item certificado
    \item certificación
    \item authorization
    \item autorizacion
    \item autorización
    \item notification
    \item notificacion
    \item notificación
    \item registration
    \item registro
    \item protocol
    \item protocolo
    \item statute
    \item estatuto
    \item ordinance
    \item decree
    \item decreto
    \item resolution
    \item resolucion
    \item official
    \item oficial
    \item office
    \item oficina
    \item department
    \item departamento
    \item service
    \item servicio
    \item servizo
    \item unit
    \item unidad
    \item unidade
\end{itemize}
\end{multicols}
\section{Saída OCR da táboa de exemplo}
\label{app:OCR}
\begin{tcolorbox}[
    title=Texto OCR,
    colback=rosa,
    colframe=rosa!80!black,
    fontupper=\small,
    fonttitle=\small\bfseries,
    breakable,  % Permite dividir en varias páginas si es necesario
    boxrule=0.5pt,
    arc=3pt,
    outer arc=3pt,
    left=5pt,
    right=5pt,
    top=3pt,
    bottom=3pt,
    before skip=10pt,
    after skip=10pt,
    width=\textwidth,  % Asegura que no se salga del ancho de página
    coltitle=black]
\begin{spacing}{0.85}  % Reduce el interlineado ligeramente
[Tipos de usuarios

N? de documentos en préstamo

Días de préstamo

Renovaciones

Reservas

GRUPO 1

Estudiantado de Grado de centros propios y adscritos

Estudiantado de programa de movilidad (Erasmus, Sicue-Séneca)

Estudiantado de doble Grado, de simultaneidad 10 10 días Indefinidas | Límite 10 docs.
Estudiantado de grados interuniversitarios
Estudiantado da Universidad Séntor
GRUPO 2
Estudiantado de MASTERES e posgrados propios, incluyendo los
dela Fundación Universidade da Coruña . ,
15 21 días Indefinidas | Límite 15 docs.
Estudiantado de Trabajos de fin de grado.
Personal de administración en servicio (PTGAS)
GRUPO 3
Estudiantado de Doctorado 35 30 días Indefinidas | Límite 35 does.
Personal investigador visitante; visitant tant
'ersonal investigador visitante: visitante senior] visitante 35 30 días indefinidas | Límite 38 docs
predoctoral o postdactoral
GRUPO 4
Becarios/as de investigación Curso académico
Personal contratado investigador (cada biblioteca podrá excluir
35 de este tipo de préstamo Indefinidas | Límite 35 docs.
documentos par razón de uso y
disponibilidad)
GRUPO 5
PDI da UDC (incluyendo centros adscritos), de la Fundación UDC Curso académico
Profesorado emérito, jubilado incentivado y honorario (cada biblioteca podrá excluir
Profesorado visitante 100 de este tipo de préstamo Indefíridas | Límite 100 docs.
Lectores/as documentos par razón de uso y
disponibilidad)
GRUPO 6
Cual ! idad taria de la UDC
cualquier persona ajena a la comunidad universitaria dela 6 10 días indefinidas | Límite 6 docs

que sea autorizada por la Biblioteca Universitaria


\end{spacing}
\end{tcolorbox}

\section{Progresión de páxinas \textit{retrieveadas} ao longo do crawleo}
\label{fig:crawling}
\begin{figure}
  \centering
    \includegraphics[width=0.9\textwidth]{imaxes/crawl_speed.png}
    \caption{Progresión da eficiencia do \textit{crawler} ao longo do tempo}
    \label{fig:crawlerRun}
\end{figure}

\chapter{Análise do Corpus}

\section{Distribución de tipos de ficheiros}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{imaxes/formatos_documentos.png}
    \caption{Distribución de tipos de ficheiros no corpus}
    \label{fig:file_types_distribution}
\end{figure}

\section{Palabras máis frecuentes}

\begin{table}[H]
\centering
\begin{tabular}{rllr}
\toprule
\textbf{Pos.} & \textbf{Palabra} & & \textbf{Frecuencia} \\
\midrule
1  & de     & - & 385,703 \\
2  & a      & - & 113,823 \\
3  & en     & - & 102,598 \\
4  & la     & - & 102,014 \\
5  & e      & - & 90,196 \\
6  & y      & - & 77,587 \\
7  & que    & - & 73,069 \\
8  & o      & - & 57,323 \\
9  & da     & - & 49,975 \\
10 & el     & - & 48,951 \\
11 & para   & - & 43,204 \\
12 & do     & - & 40,254 \\
13 & se     & - & 35,387 \\
14 & los    & - & 35,200 \\
15 & del    & - & 33,793 \\
16 & las    & - & 29,507 \\
17 & por    & - & 27,315 \\
18 & no     & - & 25,683 \\
19 & con    & - & 25,326 \\
20 & udc    & - & 25,182 \\
\bottomrule
\end{tabular}
\caption{Top 20 palabras máis frecuentes no corpus}
\label{tab:top20}
\end{table}

\section{Palabras menos frecuentes}

\begin{table}[H]
\centering
\begin{tabular}{rllr}
\toprule
\textbf{Pos.} & \textbf{Palabra} & & \textbf{Frecuencia} \\
\midrule
1  & ricondo              & - & 1 \\
2  & acompahamento        & - & 1 \\
3  & recomendaciéns       & - & 1 \\
4  & aproximacién         & - & 1 \\
5  & nosum                & - & 1 \\
6  & climent              & - & 1 \\
7  & vengut               & - & 1 \\
8  & empar                & - & 1 \\
9  & retransmision        & - & 1 \\
10 & toxicoloxia          & - & 1 \\
11 & landeira             & - & 1 \\
12 & angelines            & - & 1 \\
13 & psicoloxicos         & - & 1 \\
14 & mase                 & - & 1 \\
15 & lameiras             & - & 1 \\
16 & cartelixornadasumisionquimicaevs & - & 1 \\
17 & conciliacións        & - & 1 \\
18 & conciliaciones       & - & 1 \\
19 & operatoria           & - & 1 \\
20 & extranjeos           & - & 1 \\
\bottomrule
\end{tabular}
\caption{Top 20 palabras menos frecuentes no corpus}
\label{tab:bottom20}
\end{table}

\section{Resultados validación RAGsystem}
\subsection{\textit{Heatmaps} de varias métricas}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imaxes/heatmaps/metricas_heatmaps.png}
    \caption{Heatmaps das métricas de validación}
    \label{fig:heatmap_metrics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imaxes/heatmaps/metricas_combinadas_heatmap.png}
    \caption{Heatmap combinado de todas las métricas normalizadas}
    \label{fig:heatmap_combined}
\end{figure}

\subsection{\textit{Surface plots} de varias métricas}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imaxes/heatmaps/surface_plots.png}
    \caption{Surface plots de Faithfulness y Relevance}
    \label{fig:surface_plots}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imaxes/heatmaps/surface_plots_contour.png}
    \caption{Surface plots con contornos de Faithfulness y Relevance}
    \label{fig:surface_plots_contour}
\end{figure}


\subsection{Comparación de métricas para diferentes valores de perso de BM25 co mellor \textit{chunk size} (2048)}
\begin{table}[h]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\caption{Faithfulness según BM25 Weight (Chunk Size = 2048)}
\label{tab:faithfulness_2048}
\begin{tabular}{cc}
\hline
\textbf{BM25 Weight} & \textbf{Faithfulness} \\
\hline
0.0 & 0.985 \\
0.2 & 1.000 \\
0.4 & 0.970 \\
0.8 & 0.963 \\
1.2 & 0.963 \\
1.6 & 0.968 \\
\hline
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\caption{Relevance según BM25 Weight (Chunk Size = 2048)}
\label{tab:relevance_2048}
\begin{tabular}{cc}
\hline
\textbf{BM25 Weight} & \textbf{Relevance} \\
\hline
0.0 & 0.612 \\
0.2 & 0.628 \\
0.4 & 0.656 \\
0.8 & 0.868 \\
1.2 & 0.896 \\
1.6 & 0.884 \\
\hline
\end{tabular}
\end{minipage}
\end{table}
\end{document}
