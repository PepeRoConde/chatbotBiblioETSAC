\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[galician]{babel}
\usepackage{csquotes}

\usepackage{array}
\usepackage{booktabs}            % \toprule \midrule \bottomrule
\usepackage{tabularx}   
\usepackage{verbatim}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{multicol}
\tcbuselibrary{breakable}
\usepackage[style=authoryear,backend=bibtex]{biblatex}  % o style=apa, ieee, etc.
\addbibresource{referencias.bib}

\makeatletter
\let\@makecaption\relax
\makeatother

% Fonts: Palatino for body, Helvetica for headers
\usepackage{mathpazo} % Palatino
\usepackage{helvet}
\usepackage{microtype}

% Decorative initials
\input Eichenla.fd
\newcommand*\initfamily{\usefont{U}{Eichenla}{xl}{n}}
\usepackage{lettrine}

% Colors
\usepackage{xcolor}
\definecolor{techblue}{RGB}{0,51,102}
\definecolor{rosa}{RGB}{196,45,137}
\definecolor{lightgray}{RGB}{100,100,100}

% Headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textsf{\nouppercase{\leftmark}}}
\fancyhead[R]{\small\textcolor{lightgray}{\thepage}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}

% Chapter and section styling
\usepackage{titlesec}
\titleformat{\chapter}[hang]
  {\normalfont\LARGE\bfseries\sffamily\color{rosa}}
  {\thechapter.}{15pt}{}
\titlespacing*{\chapter}{0pt}{-20pt}{30pt}

\titleformat{\section}
  {\normalfont\Large\bfseries\sffamily\color{rosa}}
  {\thesection}{1em}{}

\titleformat{\subsection}
  {\normalfont\large\bfseries\sffamily}
  {\thesubsection}{1em}{}

% Better spacing
\usepackage{setspace}
\setstretch{1.1}
\setlength{\parskip}{0.4em}
\setlength{\parindent}{15pt}

% Table of contents styling
\usepackage{tocloft}
\renewcommand{\cfttoctitlefont}{\LARGE\bfseries\sffamily\color{rosa}}
\renewcommand{\cftchapfont}{\bfseries\sffamily}
\renewcommand{\cftsecfont}{\sffamily}

% Hyperlinks
\usepackage[colorlinks=true,linkcolor=rosa,citecolor=techblue,urlcolor=techblue]{hyperref}

\title{\scshape\Huge\color{rosa}Chatbot para a documentación e normativa da UDC}
\author{
  Marcelo Ferreiro Sánchez\\
  Marcos Grobas Martínez\\
  José Romero Conde
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\noindent 
O obxectivo do proxecto é desenvolver un chatbot capaz de solventar dúbidas
acerca do funcionamiento dos procesos burocráticos e de documentación da
Universidade da Coruña (UDC). Para conseguilo optamos por unha arquitectura
xenerativa aumentada por recuperación (RAG), técnica moi empleada para mellorar
o desempeño de chatbots baseados en LLM cando buscan información específica dun
dominio sobre o que o modelo de linguaxe orixinal non foi entrenado. 
\end{abstract}

\tableofcontents
\clearpage

\chapter{Introdución}
\lettrine[lines=3,lraise=0.1,findent=2pt,nindent=0em]{\initfamily{A}}{}burocracia
de calquera campo pode chegar a ser moi complexa e pode chegar a consumir moito
tempo e recursos ás persoas que teñen que lidiar con ela. Os procesos
universitarios non son unha excepción e moitas das persoas involucradas neles
(sexan, alumnos, profesores ou persoal administrativo) os poden atopar
inabarcables ou imposibles de navegar sen axuda.

Neste contexto, apreciouse como podería ser de gran utilidade o desenvolvemento 
dun chatbot capaz de responder a preguntas relacionadas coa documentación e 
normativa da Universidade seguindo a gran tendencia da actualidade de empregar 
chatbots para diversas tarefas.

O obxectivo deste proxecto é desenvolver un chatbot que poida simplificar e 
explicar \textbf{referindo sempre ás fontes burocráticas oficiais} os procesos 
burocráticos e de documentación da Universidade da Coruña (UDC).

O sistema é resultado da unión de compoñentes e técnicas xa ben coñecidas, 
sempre tendo en mente o obxectivo a cumprir. Polo tanto, tratouse máis ben 
dunha tarefa de aplicación e adaptación de técnicas e ferramentas xa existentes 
nun caso particular, antes que de deseño ou resolución de novos problemas.

Se ben este traballo está circunscrito no contexto da asignatura de Técnicas 
Avanzadas de Procesamiento de Linguaxe Natural (TAPLN), debido á súa natureza 
e obxectivo, gran parte do tempo invertido no proxecto dedicouse á tarefa de 
recolección de información para o RAG.

Non existindo unha 'base de datos' oficial da UDC sobre a que un usuario poda 
descargar toda a documentación relativa ao centro, senón que atópase esta 
repartida nas páxinas web dos seus diferentes centros, foi necesario deseñar 
unha solución que nos permita extraela a partir dos seus portais oficiais.

Este tipo de tarefas non son novas no mundo da informática, de feito pertencen 
a unha área máis que consolidada chamada Recuperación de Información (IR) e 
tales métodos que buscan, extraen e organizan información disposta en webs html 
son coñecidos como \emph{crawlers}, parte esencial do traballo final pois para 
asegurar que o sistema sexa capaz de responder á maioría de dúbidas dos 
usuarios, é necesario ter un corpus de toda información mínimamente relevante 
acerca do funcionamento interno da universidade.




\chapter{Solución proposta}
\lettrine[lines=3,lraise=0.1,findent=2pt,nindent=0em]{\initfamily{M}}{}ostrarase
nesta sección a arquitectura xeral do sistema e posteriormente describirase cada
un dos seus compoñentes, sendo estos principalmente o \emph{RAGsystem} e o \emph{Crawler}.

\section{A Arquitectura}
Fundamentalmente e como indicouse antes, o sistema componse de dúas partes
relativamente independientes: \emph{RAGsystem}, que será un modelo de linguaxe grande
(LLM) xa adestrado que recibirá xunto con cada consulta de usuario, unha serie
de documentos (ou fragmentos dos mesmos) para responder mellor á mesma, e por
outra parte, un sistema de \emph{crawling} que ocuparase de navegar as diversas páxinas
e portais da UDC recolectando aquelas que considere que poden conter información
relevante. \\ \\ 
Nas siguientes subseccións explicaranse en detalle o funcionamento interno de
cada parte e cómo interactúan entre elas. Optouse por unha orde de exposición
que siga o fluxo de execución do sistema, é dicir, comezando polo \emph{Crawler}
e seguindo polo \emph{RAGsystem}. Sopesouse seguir a orde cronolóxica do
desenvolvemento do sistema mais concluíuse que introduciría demasiadas exégesis
e reviraría demasiado e sen utilidade a narrativa desta memoria. De calquera
forma, explicarase a evolución das partes que máis cambios sufriron ao longo da
súa implementación.



\section{O \emph{Crawler}} Considerouse que a posesión dun bo volume de datos
sería crucial para resolver o problema a tratar, na maioría dos casos as dúbidas
burocráticas resólvense encontrando o documento adecuado, e de non telo, o
sistema veríase obrigado a comuinicarlle ao usuario que non ten acceso á tal ou
cal información, voltándose completamente inútil. \\ É por iso que a
Recuperación de Información xoga un papel crucial para o RAG. 
\subsection{Selección de páxinas relevantes}
Sendo así, o primeiro problema a solucionar é o de deseñar un bo criterio do 
que é un \textbf{documento relevante} para a aplicación. Se ben tal tarefa 
pódese complicar \textit{ad infinitum} e é un área de estudo activa aínda 
nestes días [\cite{Pezzuti_2025}], neste caso optouse por un enfoque moito máis 
sinxelo, baseándose primariamente na premisa de que esta tarefa non require de 
conxuntos masivos de datos. Só é necesario navegar un certo número de URLs 
(principalmente do directorio raíz de cada facultade), non a totalidade da web, 
polo que pódese permitir descargar páxinas e documentos potencialmente 
pouco relevantes. A información a colleitar ten un límite de tamaño finito e 
razoable: nun escenario extremo, mesmo descargando todas as URLs da UDC, 
estaríase lonxe de requirir terabytes de almacenamento, como sí acontecería 
nun \textit{crawler} de propósito xeral. \\ \\

Deste xeito, a solución proposta sitúase como un método arcaico aínda que funcional,
unha asignación de relevancia binaria (relevante ou non relevante) mediante
\textit{keywords}. Este enfoque, se ben simple, remite aos métodos de
\textit{focused crawling} pioneiros [\cite{Chakrabarti1999}] que sentaron as
bases da recuperación de información temática na web. \\ \\As palabras clave
utilizadas son fixas e propias do ámbito académico e do vocabulario burocrático,
aparecendo tanto en galego, coma en castelán e inglés (véxase o
Apéndice~\ref{app:keywords} para a lista completa).

\subsection{Procesado dos contidos de cada páxina}
Unha vez unha páxina estímase como relevante, é necesario procesar seu contido,
para este caso, procurar presentalo en texto plano, evitando ao máximo posible
decoradores. Mais nunha páxina atópase máis que texto, especialmente neste caso
de uso, moita información burocrática útil para un usario final atoparase nun
documento PDF ligado ao URL, é por iso que o \textit{crawler} tamén os procesa e
converte en texto plano listo para que posteriormente un LLM poda leelos sen
maior complicación. \\ \\ 
Non obstante, este enfoque presenta unha limitación importante: información que
se atope unicamente en imaxes (capturas de pantalla, carteis dixitalizados ou
documentos escaneados) permanece invisible para o \textit{crawler}. \\ Tal
debilidade do sistema fíxose obvia nas primeiras fases de testeo do
\textit{chatbot}, pois este non era capaz de responder a unha pregunta moi
sinxela e básica para os estudantes: "Cantos libros pódense emprestar?". Tras
investigar a casuística e comprobarse que o
\href{https://www.udc.es/es/biblioteca/servizos/prestamo/}{URL onde figura tal
información} foi efectivamente analizada polo crawler, caéuse na conta de que,
se ben a información figura nunha táboa dentro da páxina, esta está en formato
imaxe [ver \ref{figprestamos}], polo que próbase que é necesario aproveitar
mellor a información de cada páxina para cumplir os obxectivo establecido.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{imaxes/prestamos.png}
    \caption{ Táboa de préstamos}
    \label{figprestamos}
\end{figure}



\subsubsection{Procesado de imaxes mediante OCR}
O recoñecemento óptico de caracteres (OCR polas súas siglas en inglés) é unha
técnica que permite extraer texto a partir de imaxes dixitais, se ben o seu uso
máis típico é a dixitalización de documentos físicos escaneados, serve para
calquera imaxe que conteña caracteres, como é o caso. 

Neste caso, implementouse mediante a librería \textit{pytesseract} unha
interface en \textit{Python} do motor OCR \texttt{Tesseract}, creado
inicialmente por HP e mantido actualmente por Google. 

Procesar cada imaxe presente nun URL pode semexar moi custoso
computacionalmente, mais a extracción de texto mediante OCR é verdadeiramente
rápida, isto sumado a que a maioría de imaxes son irrelevantes e de moi pequeno
tamano, otorga a capacidade de poder procesar todas as presentes en calquera
documento sen apenas engadir latencia ao sistema. Como exemplo, o tempo de
procesado de OCR na imaxe da táboa de préstamos [\ref{figprestamos}] foi de
$0.6280$ segundos. \\ \\ Demostrado que é posible e relativamente sinxelo á vez
que barato procesar arquivos de imaxe, é necesario agora demostrar que tal texto
é fidedigno ao realmente existente na mesma. 


Un exemplo que demostra á perfección un caso de uso real é precisamente a táboa
mencionada anteriormente, o texto extráido en crú pódese ver no
apéndice~\ref{app:OCR}. Revisándoo, o texto extráido non asemexa nada parecido á
táboa orixinal, mais hai que ter en mente que non é necesario que sexa
intelixible para o usuario final, senón para o LLM que o debe de analizar. Unha
forma rápida de comrpobar que o chatbot poida ser capaz de entender o texto
extraído (coa complicación de que neste caso está en formato táboa) é pedirlle
mediante un prompt no portal online dalgún LLM comercial, que refaga a táboa por
nós con esa información. Fíxose tal experimento usando ChatGPT-5 e a táboa que
reconstruiu foi a seguinte:
\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{@{}p{0.18\textwidth} p{0.3\textwidth} ccc@{}}
\toprule
\textbf{Grupo} & \textbf{Tipo de usuarios} & \textbf{Nº docs} & \textbf{Días} & \textbf{Renov.} \\
\midrule
\textbf{1} & Estud. Grao (centros propios/adscritos), Mobilidade, Dobre Grao, Graos interuniv., Univ. Sénior & 10 & 10 días & Indef. (Lím. 10) \\
\midrule
\textbf{2} & Estud. másteres/posgraos propios (Fund. UDC), Estud. TFG, PTGAS & 15 & 21 días & Indef. (Lím. 15) \\
\midrule
\textbf{3} & Estud. Doutoramento, Inv. visitante/senior, Inv. predoc./posdoc & 35 & 30 días & Indef. (Lím. 35) \\
\midrule
\textbf{4} & Becarios inv., Pers. contr. inv. \textit{(Exclusión posible)} & 35 & Curso acad. & Indef. (Lím. 35) \\
\midrule
\textbf{5} & PDI UDC (inc. adscritos), Fund. UDC, Prof. emérito/xub./hon., Prof. visitante, Lectores \textit{(Exclusión posible)} & 100 & Curso acad. & Indef. (Lím. 100) \\
\midrule
\textbf{6} & Persoal externo á UDC, Pers. autoriz. Biblioteca & 6 & 10 días & Indef. (Lím. 6) \\
\bottomrule
\end{tabular}
\caption{Táboa reconstruida mediante LLM (ChatGPT-5)}
\end{table}

Se ben existe certa información que o LLM obviou por ser incompleta, a
información importante está presente, demostrando que os LLMs son capaces de
reconstruir e interpretar a información extraída mediante OCR, incluso cando
esté representada en formatos mais complexos como o é unha táboa.

\subsection{Funcionamento interno do \textit{Crawler}}

A implementación proposta usa da librería \textit{Python BeautifulSoup} para
navegar a rede e extraer información das páxinas. O proceso comeza cunha lista
de URLs semente que representa os portais principais da universidade
(concretamente a \textit{homepage} de cada facultade). Para cada URL, o sistema
descarga o contido HTML, extrae todos os enlaces e imaxes presentes na páxina, e
identifica documentos PDF que conteñan termos relacionados coa burocracia
universitaria segundo a lista de \textit{keywords} antes exposta. Os recursos
descargados almacénanse de forma organizada no sistema de ficheiros local,
mentres que un sistema de metadatos rexistra información sobre cada recurso para
optimizar futuras execucións, concretamente garda o \textit{etag} e a data de
última modificación de cada páxina, ademáis da data de descarga. 

En canto ás imaxes, aplícase OCR a todas as presentes en cada documento e
decídese se gardar o texto extraído ou non segundo se presenta máis dun mínimo
de caracteres. Todo este proceso realízase ao final de crawlear cada páxina, os
arquivos de imaxe vanse engadindo a unha pila, e esta é procesada cando xa
extraéronse as hiperreferencias a outras páxinas e os documentos PDF. 

Para acelerar o proceso de \textit{crawling} aplicáronse técnicas de
programación concurrente. É posible establecer un número de traballadores aos
cales se lles asignará unha URL como base desde onde iniciar o crawler, deste
xeito, é posible ter múltiples crawlers simultáneamente navegando a rede da
universidade. Para non xerar problemas de lectura/escritura nos ficheiros onde
se escriben os metadatos ou o rexistro de páxinas visitadas, implementáronse
\textit{locks}, deste xeito dous \textit{crawlers} non podrán escribir á vez nun
mesmo documento. Implementar esta técnica foi de gran utilidade, pois a
navegación e descarga de páxinas web vai fancéndose máis lenta segundo o tempo
de \textit{crawling}, pois é mais difícil atopar páxinas relevantes non
visitadas, no apéndice~\ref{fig:crawlerRun} pódense consultar gráficas acerca
dun crawleo que o confirman.

\section{Análise dos datos obtidos mediante o \textit{Crawler}} Tras un facer un
\textit{crawl} sobre as  \textit{homepages} das facultades e escolas da UDC,
obtiveronse un total de 4966 documentos, dos cales 2668 son páxinas
\textit{HTML} e 2298 son documentos \textit{PDF} (véxase~\ref{fig:file_types_distribution}).

Debido a que esta práctica está tan enlazada coas técnicas de Recuperación da
Información clásicas, pareceu interesante realizar unha análise máis exploratoria
típica no sector. En concreto estudar a distribución das palabras do vocabulario
e lonxitude dos documentos: 

\begin{table}[h]
\centering
\caption{Estadísticas del corpus}
\label{tab:corpus_stats}
\begin{tabular}{lr}
\hline
\textbf{Métrica} & \textbf{Valor} \\
\hline
Arquivos procesados & 4.966 \\
Total de caracteres & 75.088.461 \\
Total de palabras & 10.164.265 \\
Tamaño del vocabulario & 73.640 \\
Palabras únicas (hapax legomena) & 15.889 \\
\hline
\end{tabular}
\end{table}

Atopamos que o \textit{corpus} obtido alcanza os 10 millóns de palabras, cun
vocabulario de 73.640 palabras ([\ref{tab:corpus_stats}]). Chama a atención o
elevado número de \textit{hapax legomena}, estas son aquelas palabras cunha
única instancia na colección completa, un 20\% do vocabulario pertence a esta
clase, o cal pode dar a pensar que unha gran cantidade de elas son faltas de
ortografía (xa sexan debidas ao erro humano de quen as escribiu ou na técnica de
OCR). 

Se ben podería ser así, un estudo posterior mostra o contrario, sen embargo, só
230 de tales palabras son faltas ortográficas, e 30 delas foron xeradas polo OCR.

O fenómeno de que unha porcentaxe tan elevada do vocabulario teña tan pouco uso
non é exclusivo deste corpus, senón que é unha propiedade universal dos idiomas
naturais descrita pola lei de Zipf. Esta lei establece que a frecuencia dunha
palabra é inversamente proporcional ao seu rango na distribución de frecuencias.
En outras palabras, un pequeno número de palabras moi frecuentes coexiste cunha
longa cola de palabras raras, como se pode apreciar na Figura~\ref{figZipf} e~\ref{figEDA}.



\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imaxes/zipf.png}
    \caption{~Frecuencia de termos en diversas linguas [\cite{wikipedia_zipf}]}
    \label{figZipf}
\end{figure}

Ademais, cúmprese o principio de Pareto: aproximadamente o 20\% das palabras
mais frecuentes do vocabulario cobren o (aproximadamente) 80\% das ocorrencias
totais, mentres que para alcanzar o 80\% das ocorrencias só se necesita o 80\%
do vocabulario (Figura~\ref{figEDA}). No caso deste corpus este principio e
aínda máis esaxerado e estremo, chegaríamos a ese 80\% só co 2.1\% do
vocabulario, e se o extenderamos ao 20\% o \textit{coverage} sería dun 97.5\%.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{imaxes/analisis_corpus.png}
    \caption{Gráficos de distribución de lonxitude de documentos e frecuencia de termos}
    \label{figEDA}
\end{figure}


A distribución das lonxitudes dos documentos (Figura~\ref{figEDA}, esquerda) 
revela unha gran cantidade de documentos moi curtos. A mediana sitúase en 4200
caracteres, mentres que a media alcanza os 15121 caracteres, o que indica unha 
distribución asimétrica con presenza de documentos extensos que elevan a media. 
Esta heteroxeneidade na lonxitude non é de extrañar pois existen moitas páxinas 
web cun contido moi limitado que só mostran unha serie de ligazóns ou documentos 
para descargar. De feito, algunhas URLs teñen a única finalidade de mostrar unha 
ou varias imaxes sen texto asociado. No presente \textit{crawleo} atopáronse 115 
páxinas completamente baleiras (0 caracteres), ben por conter unicamente imaxes 
sen texto extraíble por OCR, ben por tratarse de páxinas de redirección ou navegación 
pura.








\section{O \textit{RAGsystem}}

O sistema RAG (Retrieval-Augmented Generation) implementado constitúe o núcleo
do chatbot, combinando técnicas de recuperación de información con modelos de
linguaxe xerativos. O proceso de resposta a unha consulta desenvólvese en varias
etapas: primeiro, a pregunta do usuario convértese nun vector mediante
embeddings semánticos que capturan o seu significado; a continuación, este
vector empregase para buscar no vectorstore os documentos máis relevantes
mediante similitude coseno, recuperando os $k$ fragmentos de texto máis próximos
semanticamente á consulta. Para mellorar a calidade da recuperación, o sistema
incorpora un compoñente de reranking baseado en TF-IDF que permite reordenar os
documentos recuperados segundo a súa relevancia léxica, ou ben combinarse de
forma híbrida coas puntuacións vectoriais mediante pesos configurables. Unha vez
identificados os fragmentos máis relevantes, estes incorpóranse como contexto
nunha plantilla de prompt xunto co historial recente da conversación, permitindo
que o modelo de linguaxe (Claude, ChatGPT ou calquera compatible con
\textit{Langchain}) xere unha resposta fundamentada na documentación oficial da
universidade. O sistema mantén un historial de conversación de lonxitude
configurable que permite responder a preguntas que fan referencia a interaccións
previas, dotando ao chatbot de capacidade contextual e conversacional. Ademais,
cada documento recuperado enriquécese con metadatos que inclúen a súa puntuación
de relevancia tanto vectorial como TF-IDF, facilitando a trazabilidad e
verificación das fontes empregadas para xerar cada resposta.


\subsection{Construcción da base de vectores}
[\cite{lewis2020retrieval}] A base de vectores (vectorstore) constrúese a partir
dos documentos recollidos e previamente convertidos a texto plano (.txt) polo
\textit{crawler}. Cada documento é fragmentando despois en varios \textit
{chunks} de tamano configurable, á vez que se establece un \textit{overlap} que
especifica cantos tokens comparten os fragmentos consecutivos, súa utilidade é
impedir que existan referencias cruzadas dentro dun texto non se vexan cortadas
por terminarse o chunk, xa que nese caso o LLM non podería entender o contexto
completo.  

Para o almacenamento e indexación eficiente dos embeddings, emprégase FAISS
(\textit{Facebook AI Similarity Search})~[\cite{johnson2019billion}], unha
biblioteca optimizada para a busca de similaridade en espazos vectoriais de alta
dimensionalidade. FAISS implementa algoritmos de busca aproximada de veciños máis
próximos (ANN, \textit{Approximate Nearest Neighbors}), que permiten realizar
consultas en millóns de vectores de forma eficiente. O proceso funciona do
seguinte xeito: cada chunk de texto convértese nun vector denso (embedding)
mediante un modelo de linguaxe, estes vectores almacénanse nunha estrutura de
índice optimizada, e cando se realiza unha consulta, FAISS calcula rapidamente
os vectores máis similares utilizando medidas de distancia como a similaridade
coseno ou a distancia euclidiana~[\cite{douze2024faiss}].








\subsection{O sistema de recuperación}
O sistema polo que o RAG recibe os documentos relevantes para cada consulta
recibiu múltiples iteraciones e cambios tras varias fases de testeo e
replanteamento teórico. Trátase dun compoñente clave do sistema e non ten unha
solución trivial, pois é un área de investigación moi activa
[\cite{systematic_rag_2025}], poderíase dicir que se ben a área de IR parecía
estar algo estancada nos últimos anos, debido aos bos resultados que alcanzaron
os buscadores web, agora volve a verse moi activa grazas ao xurdimento dos
RAGsystems.

Comezou empregando un sistema de recuperación baseado exclusivamente en
similitude coseno de vectores FAISS. Mais tras comprobar que tal
\textit{approach} podía non dar os resultados esperados, pois da a mesma
importancia a todas as palabras da consulta, non ten en conta cal pode ser a
palabra clave máis relevante. A maioría das consultas (especialmente
burocráticas) van ter moitas palabras moi comúns e repetidas ao longo da
colección de documentos (proceso, documento, normativa) que poden facer que o
sistema recupere documentos pouco relevantes. 

A primeira solución implementada foi aplicar MMR (Maximal Marginal Relevance) no
canto da similaridade coseno. O fundamento detrás desta idea era que deste xeito
non desenvolverianse documentos moi redundantes entre sí, xa que no caso de
existir varios documentos moi relevantes entre sí, súa información útil para a
consulta sería menor, mais durante a experimentación comprobouse que na
realidade non se cumple tal suposición. 

É moi común ver diferentes documentos oficiais que repiten a mesma información
ou varias versións e revisións dun proceso en diferentes documentos. En tal caso
é probable que o sistema devolva un só deles, perdéndose información relevante
ou directamente indicando un proceso obsoleto, xa que prioriza diversidade ante
exhaustuvudade [\cite{carbonell1998mmr}]. Por outra banda, esta problemática
acerca da vixencia temporal dos documentos é unha das maiores problemáticas
neste proxecto (e en calquera RAG que manexe información cambiante ao longo do
tempo [\cite{grofsky2025freshness}]) e tratarase máis adiante, xa que require
modificación en diversas partes do sistema. 

Sabendo que o MMR mom é axeitado ao noso dominio, decidiuse volver cara o
enfoque orixinal, pero esta buscando a maneira de discriminar mellor entre os
términos relevantes e irrelevantes, existe un método que consegue estes
resultados desde fai tempo, o TF-IDF (Term Frequency - Inverse Document
Frequency) [\cite{sparck_jones1972}]. A forma de aplicalo inicialmente foi
utilizalo para rerankear os documentos recuperados polo FAISS, é dicir, cos n
documentos devoltos mediante a similaridade coseno, estes reordéanse segundo a
ponderación tf-idf. Prontamente caeuse na conta de que deste xeito non obtense
resultados demasiado óptimos pois simplemente cambia de orde os documentos
retrieveados polo FAISS, mais non engade novos documentos que poidan ser
relevantes e que a similaridade coseno obvia. Por iso, a solución final foi
empregar un sistema híbrido, onde se combinan as puntuacións de similitude
coseno e tf-idf mediante pesos configurables (50/50 no caso actual). Tal enfoque
é moi común nos RAGsystems actuais como no caso de \cite{regulatory_hybrid_2025}
e \cite{adobe_hybrid_2024}.







\section{Os modelos}
\section{Ferramentas usadas}

\chapter{Instalación e uso}

\chapter{Resultados}
\lettrine[lines=3,lraise=0.1,findent=2pt,nindent=0em]{\initfamily{R}}{}esults presentation here.

\chapter{Conclusions}
\lettrine[lines=3,lraise=0.1,findent=2pt,nindent=0em]{\initfamily{H}}{}ola castro


\printbibliography
\appendix

\chapter{Máis información acerca do \textit{crawler}}

\section{\textit{Keywords} utilizadas polo crawler}
\label{app:keywords}

A continuación móstrase o conxunto completo de palabras clave utilizadas
polo \textit{crawler} para determinar a relevancia das páxinas:

\begin{multicols}{3}
\small
\begin{itemize}
    \item regulation
    \item reglamento
    \item normativa
    \item procedure
    \item procedimiento
    \item proceso
    \item form
    \item formulario
    \item solicitud
    \item guideline
    \item guia
    \item manual
    \item policy
    \item politica
    \item norma
    \item enrollment
    \item matricula
    \item inscripcion
    \item administrative
    \item administrativo
    \item academic
    \item academico
    \item calendar
    \item calendario
    \item syllabus
    \item programa
    \item requirements
    \item requisitos
    \item regulamento
    \item regulación
    \item procedemento
    \item solicitude
    \item guía
    \item docente
    \item asignatura
    \item política
    \item matrícula
    \item inscrición
    \item académico
    \item convocatoria
    \item prazo
    \item prazos
    \item documentación
    \item tramite
    \item trámite
    \item ordenanza
    \item resolución
    \item circular
    \item instrucións
    \item instrucciones
    \item bases
    \item anexo
    \item catalog
    \item catalogo
    \item catálogo
    \item library
    \item biblioteca
    \item collection
    \item coleccion
    \item colección
    \item acquisition
    \item adquisicion
    \item adquisición
    \item loan
    \item prestamo
    \item préstamo
    \item reserve
    \item reserva
    \item interlibrary
    \item interbibliotecario
    \item reference
    \item referencia
    \item circulation
    \item circulacion
    \item circulación
    \item periodical
    \item periodico
    \item periódico
    \item journal
    \item revista
    \item archive
    \item archivo
    \item arquivos
    \item repository
    \item repositorio
    \item classification
    \item clasificacion
    \item clasificación
    \item indexing
    \item indexacion
    \item indexación
    \item cataloging
    \item catalogacion
    \item catalogación
    \item dewey
    \item isbn
    \item issn
    \item bibliographic
    \item bibliografico
    \item bibliográfico
    \item holdings
    \item fondos
    \item serials
    \item publicacions seriadas
    \item special collections
    \item coleccions especiais
    \item reading room
    \item sala de lectura
    \item stacks
    \item depósito
    \item microfilm
    \item microficha
    \item digital library
    \item biblioteca dixital
    \item opac
    \item marc
    \item application
    \item deadline
    \item plazo
    \item documentation
    \item documentacion
    \item certification
    \item certificado
    \item certificación
    \item authorization
    \item autorizacion
    \item autorización
    \item notification
    \item notificacion
    \item notificación
    \item registration
    \item registro
    \item protocol
    \item protocolo
    \item statute
    \item estatuto
    \item ordinance
    \item decree
    \item decreto
    \item resolution
    \item resolucion
    \item official
    \item oficial
    \item office
    \item oficina
    \item department
    \item departamento
    \item service
    \item servicio
    \item servizo
    \item unit
    \item unidad
    \item unidade
\end{itemize}
\end{multicols}
\section{Saída OCR da táboa de exemplo}
\label{app:OCR}
\begin{tcolorbox}[
    title=Texto OCR,
    colback=rosa,
    colframe=rosa!80!black,
    fontupper=\small,
    fonttitle=\small\bfseries,
    breakable,  % Permite dividir en varias páginas si es necesario
    boxrule=0.5pt,
    arc=3pt,
    outer arc=3pt,
    left=5pt,
    right=5pt,
    top=3pt,
    bottom=3pt,
    before skip=10pt,
    after skip=10pt,
    width=\textwidth,  % Asegura que no se salga del ancho de página
    coltitle=black]
\begin{spacing}{0.85}  % Reduce el interlineado ligeramente
[Tipos de usuarios

N? de documentos en préstamo

Días de préstamo

Renovaciones

Reservas

GRUPO 1

Estudiantado de Grado de centros propios y adscritos

Estudiantado de programa de movilidad (Erasmus, Sicue-Séneca)

Estudiantado de doble Grado, de simultaneidad 10 10 días Indefinidas | Límite 10 docs.
Estudiantado de grados interuniversitarios
Estudiantado da Universidad Séntor
GRUPO 2
Estudiantado de MASTERES e posgrados propios, incluyendo los
dela Fundación Universidade da Coruña . ,
15 21 días Indefinidas | Límite 15 docs.
Estudiantado de Trabajos de fin de grado.
Personal de administración en servicio (PTGAS)
GRUPO 3
Estudiantado de Doctorado 35 30 días Indefinidas | Límite 35 does.
Personal investigador visitante; visitant tant
'ersonal investigador visitante: visitante senior] visitante 35 30 días indefinidas | Límite 38 docs
predoctoral o postdactoral
GRUPO 4
Becarios/as de investigación Curso académico
Personal contratado investigador (cada biblioteca podrá excluir
35 de este tipo de préstamo Indefinidas | Límite 35 docs.
documentos par razón de uso y
disponibilidad)
GRUPO 5
PDI da UDC (incluyendo centros adscritos), de la Fundación UDC Curso académico
Profesorado emérito, jubilado incentivado y honorario (cada biblioteca podrá excluir
Profesorado visitante 100 de este tipo de préstamo Indefíridas | Límite 100 docs.
Lectores/as documentos par razón de uso y
disponibilidad)
GRUPO 6
Cual ! idad taria de la UDC
cualquier persona ajena a la comunidad universitaria dela 6 10 días indefinidas | Límite 6 docs

que sea autorizada por la Biblioteca Universitaria


\end{spacing}
\end{tcolorbox}

\section{Progresión de páxinas \textit{retrieveadas} ao longo do crawleo}
\label{fig:crawling}
\begin{figure}
  \centering
    \includegraphics[width=0.9\textwidth]{imaxes/crawl_speed.png}
    \caption{Progresión da eficiencia do \textit{crawler} ao longo do tempo}
    \label{fig:crawlerRun}
\end{figure}

\chapter{Análise do Corpus}

\section{Distribución de tipos de ficheiros}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{imaxes/formatos_documentos.png}
    \caption{Distribución de tipos de ficheiros no corpus}
    \label{fig:file_types_distribution}
\end{figure}

\section{Palabras máis frecuentes}

\begin{table}[H]
\centering
\begin{tabular}{rllr}
\toprule
\textbf{Pos.} & \textbf{Palabra} & & \textbf{Frecuencia} \\
\midrule
1  & de     & - & 385,703 \\
2  & a      & - & 113,823 \\
3  & en     & - & 102,598 \\
4  & la     & - & 102,014 \\
5  & e      & - & 90,196 \\
6  & y      & - & 77,587 \\
7  & que    & - & 73,069 \\
8  & o      & - & 57,323 \\
9  & da     & - & 49,975 \\
10 & el     & - & 48,951 \\
11 & para   & - & 43,204 \\
12 & do     & - & 40,254 \\
13 & se     & - & 35,387 \\
14 & los    & - & 35,200 \\
15 & del    & - & 33,793 \\
16 & las    & - & 29,507 \\
17 & por    & - & 27,315 \\
18 & no     & - & 25,683 \\
19 & con    & - & 25,326 \\
20 & udc    & - & 25,182 \\
\bottomrule
\end{tabular}
\caption{Top 20 palabras máis frecuentes no corpus}
\label{tab:top20}
\end{table}

\section{Palabras menos frecuentes}

\begin{table}[H]
\centering
\begin{tabular}{rllr}
\toprule
\textbf{Pos.} & \textbf{Palabra} & & \textbf{Frecuencia} \\
\midrule
1  & ricondo              & - & 1 \\
2  & acompahamento        & - & 1 \\
3  & recomendaciéns       & - & 1 \\
4  & aproximacién         & - & 1 \\
5  & nosum                & - & 1 \\
6  & climent              & - & 1 \\
7  & vengut               & - & 1 \\
8  & empar                & - & 1 \\
9  & retransmision        & - & 1 \\
10 & toxicoloxia          & - & 1 \\
11 & landeira             & - & 1 \\
12 & angelines            & - & 1 \\
13 & psicoloxicos         & - & 1 \\
14 & mase                 & - & 1 \\
15 & lameiras             & - & 1 \\
16 & cartelixornadasumisionquimicaevs & - & 1 \\
17 & conciliacións        & - & 1 \\
18 & conciliaciones       & - & 1 \\
19 & operatoria           & - & 1 \\
20 & extranjeos           & - & 1 \\
\bottomrule
\end{tabular}
\caption{Top 20 palabras menos frecuentes no corpus}
\label{tab:bottom20}
\end{table}

\section{Resultados validación RAGsystem}
\subsection{\textit{Heatmaps} de varias métricas}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imaxes/heatmaps/metricas_heatmaps.png}
    \caption{Heatmaps das métricas de validación}
    \label{fig:heatmap_metrics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imaxes/heatmaps/metricas_combinadas_heatmap.png}
    \caption{Heatmap combinado de todas las métricas normalizadas}
    \label{fig:heatmap_combined}
\end{figure}

\subsection{\textit{Surface plots} de varias métricas}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imaxes/heatmaps/surface_plots.png}
    \caption{Surface plots de Faithfulness y Relevance}
    \label{fig:surface_plots}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imaxes/heatmaps/surface_plots_contour.png}
    \caption{Surface plots con contornos de Faithfulness y Relevance}
    \label{fig:surface_plots_contour}
\end{figure}


\subsection{Comparación de métricas para diferentes valores de perso de BM25 co mellor \textit{chunk size} (2048)}
\begin{table}[h]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\caption{Faithfulness según BM25 Weight (Chunk Size = 2048)}
\label{tab:faithfulness_2048}
\begin{tabular}{cc}
\hline
\textbf{BM25 Weight} & \textbf{Faithfulness} \\
\hline
0.0 & 0.985 \\
0.2 & 1.000 \\
0.4 & 0.970 \\
0.8 & 0.963 \\
1.2 & 0.963 \\
1.6 & 0.968 \\
\hline
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\caption{Relevance según BM25 Weight (Chunk Size = 2048)}
\label{tab:relevance_2048}
\begin{tabular}{cc}
\hline
\textbf{BM25 Weight} & \textbf{Relevance} \\
\hline
0.0 & 0.612 \\
0.2 & 0.628 \\
0.4 & 0.656 \\
0.8 & 0.868 \\
1.2 & 0.896 \\
1.6 & 0.884 \\
\hline
\end{tabular}
\end{minipage}
\end{table}
\end{document}
