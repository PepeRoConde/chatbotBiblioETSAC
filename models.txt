####################
# LLMs (Xeración de Texto) - Optimizado para RTX 4070
####################

# Modelos Lixeiros (Máis Rápidos)
TinyLlama/TinyLlama-1.1B-Chat-v1.0
- VRAM: <2GB (4-bit)
- Velocidade: ~100 tokens/seg
- Ideal para: Prototipado rápido, chatbots simples

# Modelos 7B (Punto Doce)
mistralai/Mistral-7B-Instruct-v0.2
- VRAM: 6GB (4-bit)
- Velocidade: ~30 tokens/seg
- Formatos recomendados: AWQ/GPTQ/GGUF

HuggingFaceH4/zephyr-7b-beta
- VRAM: 6.5GB (4-bit)
- Puntos fortes: Escrita creativa, instrucións complexas

# Modelos Bilingües (Galego/Español)
bertin-project/bertin-gpt-j-6B-8bit # no va
mrm8488/bertin-gpt-j-6B-ES-8bit
- VRAM: 6.8GB (8-bit)
- Adestrado con corpus galego/español
- Usar con: galician-stemmer en langchain

PlanTL-GOB-ES/gpt2-large-bne
- VRAM: 5GB (4-bit)
- Manexa galego con prompts adecuados
- Ideal para: Documentos legais/goberno

# Cuantizados
TheBloke/Mistral-7B-Instruct-v0.2-GGUF
- Recomendado: Q5_K_M (para mellor español)
- Engadir prompt: "Responde en galego se posible"

########################
# Modelos de Embedding (RAG en Galego)
########################

# Especializados Galego/Español
HiTZ/xlm-r-large-en-es
- VRAM: 3.2GB (FP16)
- Recuperación cruzada: es-gl
- Tamaño do batch: 16-32

sentence-transformers/paraphrase-multilingual-mpnet-base-v2
- VRAM: 1.5GB (FP16)
- Probado con textos galegos
- Opción por defecto para RAG

# Multilingüe
intfloat/multilingual-e5-large
- VRAM: 2.8GB (FP16)
- Prefixos: "query: " / "passage: "
- Ideal para corpus mesturados

# Nova Opción
PlanTL-GOB-ES/roberta-large-bne
- VRAM: 2.1GB (FP16)
- Adestrado con: noticias ES+GL+PT
- Ideal para: Actualidade/política

########################
# CONSELLOS PARA RAG EN GALEGO
########################

Pre-procesamento:
1. Usar galician-stemmer
2. Filtrado por metadatos de idioma
3. Normalizar diacríticos (ñ → n)

Recuperación:
- Busca híbrida (dense + lexical)
- Ponderar documentos en galego
- Usar "langdetect" para auto-filtrado

Xeración:
- Prompt do sistema: "Responde en galego"
- Exemplos few-shot en galego
- Post-procesado con: galician_tokenizer

########################
# OPTIMIZACIÓNs PARA RTX 4070
########################
- Modelos 7B: Usar Q4_K_S
- Activar flash_attention_2
- vLLM con tokenizer galego
- ONNX para embeddings
- Batch ideal: 8-16 (xeración), 32-64 (embeddings)
